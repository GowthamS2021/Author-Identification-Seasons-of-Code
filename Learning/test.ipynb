{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wo', \"n't\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(\"won't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "word_stemmer = PorterStemmer()\n",
    "word_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['auibrauinoginw', \"''\", 'neinrfwi', \"''\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tw = TreebankWordTokenizer()\n",
    "tw.tokenize('auibrauinoginw\"neinrfwi\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['auibrauinoginw', '\"', 'neinrfwi', '\"', 'feaoefm']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "wp = WordPunctTokenizer()\n",
    "wp.tokenize('auibrauinoginw\"neinrfwi\"feaoefm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['webbfr anfei iengiwn.', 'infqwinfr inweirn \"iwnri\" iqefin']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize('webbfr anfei iengiwn. infqwinfr inweirn \"iwnri\" iqefin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"won't\", 'is', 'a', 'contradiction']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "rt1 = RegexpTokenizer(\"[\\w']+\",gaps=False)\n",
    "rt1.tokenize(\"won't is a contradiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', ' ', ' ']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt2 = RegexpTokenizer(\"[\\w']+\",gaps=True)\n",
    "rt2.tokenize(\"won't is a contradiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nGuy: How old are you?', 'Hipster girl: You know, I never answer that question.', \"Because to me, it's about\\nhow mature you are, you know?\", 'I mean, a fourteen year old could be more mature\\nthan a twenty-five year old, right?', \"I'm sorry, I just never answer that question.\", \"Guy: But, uh, you're older than eighteen, right?\", 'Hipster girl: Oh, yeah.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "txt = webtext.raw('/home/gowtham/Desktop/soc/mentee/text.txt')\n",
    "# print(txt)\n",
    "st = PunktSentenceTokenizer(txt)\n",
    "st1 = st.tokenize(txt)\n",
    "print(st1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nGuy: How old are you?', 'Hipster girl: You know, I never answer that question.', \"Because to me, it's about\\nhow mature you are, you know?\", 'I mean, a fourteen year old could be more mature\\nthan a twenty-five year old, right?', \"I'm sorry, I just never answer that question.\", \"Guy: But, uh, you're older than eighteen, right?\", 'Hipster girl: Oh, yeah.']\n"
     ]
    }
   ],
   "source": [
    "st2 = sent_tokenize(txt)\n",
    "print(st2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "medication_pattern = re.compile(r'\\b[A-Z][a-z]*[\\-\\s]?[0-9]+?[a-z]*\\b')\n",
    "\n",
    "tokenizer = RegexpTokenizer(medication_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aspirin 81', 'Lipitor 20']\n"
     ]
    }
   ],
   "source": [
    "text = \"The patient was prescribed Aspirin 81 mg and Lipitor 20 mg daily.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'anfi']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "e_s = set(stopwords.words('english'))\n",
    "words = ['I','a','am','anfi']\n",
    "l = [word for word in words if word not in e_s]\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog.n.01\n",
      "a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
      "['the dog barked all night']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "syn = wn.synsets('dog')[0]\n",
    "print(syn.name())\n",
    "print(syn.definition())\n",
    "print(syn.examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n",
      "[Synset('bitch.n.04'), Synset('dog.n.01'), Synset('fox.n.01'), Synset('hyena.n.01'), Synset('jackal.n.01'), Synset('wild_dog.n.01'), Synset('wolf.n.01')]\n",
      "[Synset('entity.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(syn.hypernyms())\n",
    "print(syn.hypernyms()[0].hyponyms())\n",
    "print(syn.root_hypernyms())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('dog.n.01.dog'), Lemma('dog.n.01.domestic_dog'), Lemma('dog.n.01.Canis_familiaris')]\n",
      "3\n",
      "Canis_familiaris\n"
     ]
    }
   ],
   "source": [
    "lemmas = syn.lemmas()\n",
    "print(lemmas)\n",
    "print(len(lemmas))\n",
    "print(lemmas[2].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('bad.a.01.bad')\n",
      "having undesirable or negative qualities\n"
     ]
    }
   ],
   "source": [
    "syn1 = wn.synset('good.a.01')\n",
    "ant1 = syn1.lemmas()[0].antonyms()[0]\n",
    "print(ant1)\n",
    "print(ant1.synset().definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "eat\n",
      "eat\n",
      "eat\n",
      "believ\n",
      "believes\n",
      "believ\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer,RegexpStemmer,SnowballStemmer\n",
    "Ls = LancasterStemmer()\n",
    "print(Ls.stem('eats'))\n",
    "Rs = RegexpStemmer('ing')\n",
    "print(Rs.stem('eating'))\n",
    "Ss = SnowballStemmer('english')\n",
    "print(Ss.stem('eating'))\n",
    "print(Ss.stem('eats'))\n",
    "print(Ss.stem('believes'))\n",
    "print(Rs.stem('believes'))\n",
    "print(Ls.stem('believes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating\n",
      "belief\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "l = WordNetLemmatizer()\n",
    "print(l.lemmatize('eating'))\n",
    "print(l.lemmatize('believes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will not do it\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "R_patterns = [\n",
    "   (r'won\\'t', 'will not'),\n",
    "   (r'can\\'t', 'cannot'),\n",
    "   (r'i\\'m', 'i am'),\n",
    "   (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "   (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "   (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "   (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "   (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "]\n",
    "\n",
    "class REReplacer(object):\n",
    "    def __init__(self,pattern=R_patterns):\n",
    "            self.pattern = [(re.compile(regex),repl) for (regex,repl) in pattern]\n",
    "    def replace(self,txt):\n",
    "        s = txt\n",
    "        for (pattern,repl) in self.pattern:\n",
    "            s= re.sub(pattern,repl,s)\n",
    "        return s\n",
    "\n",
    "r = REReplacer()\n",
    "print(r.replace(\"I won't do it\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'wo', \"n't\", 'be', 'able', 'to', 'do', 'this', 'now']\n",
      "['I', 'will', 'not', 'be', 'able', 'to', 'do', 'this', 'now']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(\"I won't be able to do this now\"))\n",
    "print(word_tokenize(r.replace(\"I won't be able to do this now\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "class Repremoval(object):\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "    def replace(self, word):\n",
    "        if wn.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        else:\n",
    "            return repl_word\n",
    "        \n",
    "R = Repremoval()\n",
    "print(R.replace('hiiiiiiii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv,yaml\n",
    "\n",
    "class SynReplacer(object):\n",
    "    def __init__(self,word_map):\n",
    "        self.word_map = word_map\n",
    "    def replace(self,word):\n",
    "        return self.word_map.get(word,word)\n",
    "\n",
    "class CSV_SynReplacer(SynReplacer):\n",
    "    def __init__(self,file):\n",
    "        word_map = {}\n",
    "        for line in csv.reader(open(file)):\n",
    "            word,syn = line\n",
    "            word_map[word] = syn\n",
    "        super(CSV_SynReplacer,self).__init__(word_map)\n",
    "    \n",
    "class YAML_SynReplacer(SynReplacer):\n",
    "    def __init__(self,file):\n",
    "        word_map = yaml.load(open(file))\n",
    "        super(YAML_SynReplacer,self).__init__(word_map)\n",
    "\n",
    "class AntReplacer(object):\n",
    "    def replace(self,word,pos=None):\n",
    "        antonyms = set()\n",
    "        for syn in wn.synsets(word,pos):\n",
    "            for lemma in syn.lemmas():\n",
    "                for ant in lemma.antonyms():\n",
    "                    antonyms.add(ant)\n",
    "        if len(antonyms) == 1:\n",
    "            return antonyms.pop()\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def sub(self,sent):\n",
    "        i,l,words = 0,len(sent),[]\n",
    "        while(i<l):\n",
    "            word = sent[i]\n",
    "            if word == 'not' and i+1 < l:\n",
    "                ant = self.replace(sent[i+1])\n",
    "                if ant:\n",
    "                    words.append(ant)\n",
    "                    i += 1\n",
    "            else :\n",
    "                words.append(word)\n",
    "            i+=1\n",
    "        return words        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
